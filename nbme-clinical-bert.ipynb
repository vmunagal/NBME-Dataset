{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nimport tokenizers\nimport torch.nn as nn\nimport torch \nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm \nfrom ast import literal_eval\nimport time\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel , AutoTokenizer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:31:56.330339Z","iopub.execute_input":"2022-10-06T10:31:56.330889Z","iopub.status.idle":"2022-10-06T10:32:02.892112Z","shell.execute_reply.started":"2022-10-06T10:31:56.330780Z","shell.execute_reply":"2022-10-06T10:32:02.891334Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class config:\n    MAX_LEN = 312\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16 \n    EPOCHS = 2\n    model = \"Tsubasaz/clinical-bert-base-128\" # pretrained model on clinical notes \n    MODEL_PATH = \"model.bin\"\n    TOKENIZER = AutoTokenizer.from_pretrained(model)\n    DROPOUT = 0.2\n    MAX_GRAD_NORM = 1.0\n    LEARNING_RATE = 1e-5","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:02.893740Z","iopub.execute_input":"2022-10-06T10:32:02.894006Z","iopub.status.idle":"2022-10-06T10:32:15.964656Z","shell.execute_reply.started":"2022-10-06T10:32:02.893972Z","shell.execute_reply":"2022-10-06T10:32:15.963842Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/321 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de02400d3074b1d88fd370674dc17c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b814c2fac3645c1bf14f6fa342df1bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3409f3cc7ae4949a80a98efd3a2c79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc8f04b401794f17ba4fe95d40d652f7"}},"metadata":{}}]},{"cell_type":"code","source":"BASE_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nfeatures_df = pd.read_csv(BASE_PATH + \"features.csv\")\npatient_notes_df = pd.read_csv(BASE_PATH + \"patient_notes.csv\")\ntrain_df = pd.read_csv(BASE_PATH + \"train.csv\")\ntest_df = pd.read_csv(BASE_PATH + \"test.csv\")\nsubmission_df = pd.read_csv(BASE_PATH + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:15.966005Z","iopub.execute_input":"2022-10-06T10:32:15.966256Z","iopub.status.idle":"2022-10-06T10:32:16.699196Z","shell.execute_reply.started":"2022-10-06T10:32:15.966222Z","shell.execute_reply":"2022-10-06T10:32:16.698433Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(train_df, features_df, on=['feature_num','case_num'], how='inner')\ndf =pd.merge(df, patient_notes_df, on=['pn_num','case_num'], how='inner')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:16.701259Z","iopub.execute_input":"2022-10-06T10:32:16.701531Z","iopub.status.idle":"2022-10-06T10:32:16.747497Z","shell.execute_reply.started":"2022-10-06T10:32:16.701496Z","shell.execute_reply":"2022-10-06T10:32:16.746814Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num  \\\n0  00016_000         0      16            0   \n1  00016_001         0      16            1   \n2  00016_002         0      16            2   \n3  00016_003         0      16            3   \n4  00016_004         0      16            4   \n\n                                 annotation              location  \\\n0          ['dad with recent heart attcak']           ['696 724']   \n1             ['mom with \"thyroid disease']           ['668 693']   \n2                        ['chest pressure']           ['203 217']   \n3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n4  ['felt as if he were going to pass out']           ['222 258']   \n\n                                        feature_text  \\\n0  Family-history-of-MI-OR-Family-history-of-myoc...   \n1                 Family-history-of-thyroid-disorder   \n2                                     Chest-pressure   \n3                              Intermittent-symptoms   \n4                                        Lightheaded   \n\n                                          pn_history  \n0  HPI: 17yo M presents with palpitations. Patien...  \n1  HPI: 17yo M presents with palpitations. Patien...  \n2  HPI: 17yo M presents with palpitations. Patien...  \n3  HPI: 17yo M presents with palpitations. Patien...  \n4  HPI: 17yo M presents with palpitations. Patien...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>annotation</th>\n      <th>location</th>\n      <th>feature_text</th>\n      <th>pn_history</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>['dad with recent heart attcak']</td>\n      <td>['696 724']</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n      <td>['mom with \"thyroid disease']</td>\n      <td>['668 693']</td>\n      <td>Family-history-of-thyroid-disorder</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n      <td>['chest pressure']</td>\n      <td>['203 217']</td>\n      <td>Chest-pressure</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>['intermittent episodes', 'episode']</td>\n      <td>['70 91', '176 183']</td>\n      <td>Intermittent-symptoms</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n      <td>['felt as if he were going to pass out']</td>\n      <td>['222 258']</td>\n      <td>Lightheaded</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.set_option(\"display.max_info_columns\", 200)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:16.748561Z","iopub.execute_input":"2022-10-06T10:32:16.748809Z","iopub.status.idle":"2022-10-06T10:32:16.752787Z","shell.execute_reply.started":"2022-10-06T10:32:16.748772Z","shell.execute_reply":"2022-10-06T10:32:16.752140Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.iloc[0,:].values","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:16.754297Z","iopub.execute_input":"2022-10-06T10:32:16.754728Z","iopub.status.idle":"2022-10-06T10:32:16.763618Z","shell.execute_reply.started":"2022-10-06T10:32:16.754692Z","shell.execute_reply":"2022-10-06T10:32:16.762908Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['00016_000', 0, 16, 0, \"['dad with recent heart attcak']\",\n       \"['696 724']\",\n       'Family-history-of-MI-OR-Family-history-of-myocardial-infarction',\n       'HPI: 17yo M presents with palpitations. Patient reports 3-4 months of intermittent episodes of \"heart beating/pounding out of my chest.\" 2 days ago during a soccer game had an episode, but this time had chest pressure and felt as if he were going to pass out (did not lose conciousness). Of note patient endorses abusing adderall, primarily to study (1-3 times per week). Before recent soccer game, took adderrall night before and morning of game. Denies shortness of breath, diaphoresis, fevers, chills, headache, fatigue, changes in sleep, changes in vision/hearing, abdominal paun, changes in bowel or urinary habits. \\r\\nPMHx: none\\r\\nRx: uses friends adderrall\\r\\nFHx: mom with \"thyroid disease,\" dad with recent heart attcak\\r\\nAll: none\\r\\nImmunizations: up to date\\r\\nSHx: Freshmen in college. Endorses 3-4 drinks 3 nights / week (on weekends), denies tabacco, endorses trying marijuana. Sexually active with girlfriend x 1 year, uses condoms'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# always use literal_eval instead of eval https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html\ndf[\"annotation\"] = [literal_eval(x) for x in df[\"annotation\"]] \ndf[\"location\"] = [literal_eval(x) for x in df[\"location\"]]","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:16.765051Z","iopub.execute_input":"2022-10-06T10:32:16.765340Z","iopub.status.idle":"2022-10-06T10:32:16.972649Z","shell.execute_reply.started":"2022-10-06T10:32:16.765297Z","shell.execute_reply":"2022-10-06T10:32:16.971942Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\npn_history_lengths = []\ntk0 = tqdm(df['pn_history'].fillna(\"\").values, total=len(df))\nfor text in tk0:\n    length = config.TOKENIZER.encode(text,add_special_tokens=False)\n        \n    pn_history_lengths.append(len(length))\nprint(f'pn_history max(lengths): {max(pn_history_lengths)}')\n\n\nfeatures_lengths=[]\ntk1 = tqdm(df['feature_text'].fillna(\"\").values, total=len(df))\nfor text in tk1:\n    length = config.TOKENIZER.encode(text,add_special_tokens=False)\n    features_lengths.append(len(length))\nprint(f'feature_text  max(lengths): {max(features_lengths)}')\n\nmax_lenght= max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\nprint(f\"max_len: {max_lenght}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:16.973763Z","iopub.execute_input":"2022-10-06T10:32:16.974041Z","iopub.status.idle":"2022-10-06T10:32:26.338032Z","shell.execute_reply.started":"2022-10-06T10:32:16.974008Z","shell.execute_reply":"2022-10-06T10:32:26.337271Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed5505b0f2941b7bd23ccdb3b1c23a6"}},"metadata":{}},{"name":"stdout","text":"pn_history max(lengths): 280\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef47bf7d217400386472ff616a89198"}},"metadata":{}},{"name":"stdout","text":"feature_text  max(lengths): 29\nmax_len: 312\n","output_type":"stream"}]},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:  \n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.339422Z","iopub.execute_input":"2022-10-06T10:32:26.339684Z","iopub.status.idle":"2022-10-06T10:32:26.344429Z","shell.execute_reply.started":"2022-10-06T10:32:26.339650Z","shell.execute_reply":"2022-10-06T10:32:26.343448Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def classLabeling(pn_history, feature_text, annotation, location, tokenizer, max_len):    ##X , Y, selected_text  \n    \n\n    location_list = loc_list_to_ints(location)   # convert the locations into a list \n\n    char_targets = [0] * len(pn_history)  # creation of character taragert you can reason below \n\n    for loc,anno in zip(location_list ,annotation):        \n        \n        len_st = int(loc[1]) - int(loc[0])\n        idx0 = None\n        idx1 = None        \n        for ind in (i for i, e in enumerate(pn_history) if (e == anno[0] and i == int(loc[0]))): # Only if the annotation start with character we are interested and look and character annotation postion match go inside the loop\n        \n            if pn_history[ind: ind+len_st] == anno.strip():\n\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                if idx0 != None and idx1 != None:\n                    for ct in range(idx0, idx1 + 1): # make character targets as \"1\" for them \n                        char_targets[ct] = 1 \n                break\n    # Tokenize the data and here we are returing the offstes which we gone use as labels which you can find below \n    tokenized_input = config.TOKENIZER.encode_plus(feature_text,pn_history,return_attention_mask=True,\n                                                  return_offsets_mapping=True,return_token_type_ids=True)\n    \n    input_ids = tokenized_input['input_ids']\n    mask = tokenized_input['attention_mask']\n    token_type_ids = tokenized_input['token_type_ids']\n    offsets = tokenized_input['offset_mapping']\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(offsets): # look for offsets \n        if sum(char_targets[offset1: offset2]) > 0: # if the lenght of the char_target for particualr target is greater than 0 then added one at that offsets \n            target_idx.append(j)\n            \n    #padding\n    padding_length = config.MAX_LEN - len(input_ids) # Since we used 312 as max_lenght incase if we less lenght we need to pad the zeros \n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n       \n    #creating label\n    ignore_idxes = np.where(np.array(token_type_ids) != 1)[0] # Bascially we use token type ids Segment token indices to indicate first and second portions of the input\n\n    label = np.zeros(len(offsets))\n    label[ignore_idxes] = 0.0 # creating a labels zero for not interested to look \n    label[target_idx] = 1.0  # label for which we are interested to look\n    return {\n    'ids': input_ids,\n    'mask': mask,\n    'token_type_ids': token_type_ids,\n    'labels': label,\n    'offsets': offsets\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.347623Z","iopub.execute_input":"2022-10-06T10:32:26.348163Z","iopub.status.idle":"2022-10-06T10:32:26.362044Z","shell.execute_reply.started":"2022-10-06T10:32:26.348128Z","shell.execute_reply":"2022-10-06T10:32:26.361290Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class NBMEDataset:\n    \n    def __init__(self,pn_history,feature_text, annotation, location):\n        self.pn_history=pn_history\n        self.feature_text=feature_text\n        self.annotation=annotation\n        self.location=location\n    def __len__(self):\n        return len(self.pn_history+self.feature_text)\n    \n    def __getitem__(self,item):\n   \n        output=classLabeling(self.pn_history[item],self.feature_text[item],self.annotation[item],self.location[item],config.TOKENIZER,config.MAX_LEN)\n        \n        return {\n            'input_ids':torch.tensor(output['ids']),\n             'mask':torch.tensor(output['mask'],dtype=torch.long),\n            'token_type_ids':torch.tensor(output['token_type_ids'],dtype=torch.long),\n            'labels':torch.tensor(output['labels'],dtype=torch.float),\n            'offsets':torch.tensor(output['offsets'],dtype=torch.long)   \n            \n        }\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.363297Z","iopub.execute_input":"2022-10-06T10:32:26.363555Z","iopub.status.idle":"2022-10-06T10:32:26.374303Z","shell.execute_reply.started":"2022-10-06T10:32:26.363521Z","shell.execute_reply":"2022-10-06T10:32:26.373480Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class NBMEModel(nn.Module):\n    def __init__(self):\n        super(NBMEModel,self).__init__()\n        self.bert=AutoModel.from_pretrained(config.model)\n        self.dropout=nn.Dropout(0.4)\n        self.linear=nn.Linear(768,1)\n        self.parameter=nn.Parameter(torch.ones(1))\n    def forward(self,ids, mask,token_ids):\n        sequence_output=self.bert(ids,attention_mask=mask, token_type_ids=token_ids)[0] # we gone take last hidden state no the model \n        output=self.dropout(sequence_output)\n        logits=self.linear(output)\n        logits = logits.squeeze(-1) \n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.375499Z","iopub.execute_input":"2022-10-06T10:32:26.375815Z","iopub.status.idle":"2022-10-06T10:32:26.386231Z","shell.execute_reply.started":"2022-10-06T10:32:26.375773Z","shell.execute_reply":"2022-10-06T10:32:26.385451Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def loss_fn(logits, labels):\n    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"mean\")\n    loss = loss_fct(logits,labels.float())\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.387677Z","iopub.execute_input":"2022-10-06T10:32:26.387963Z","iopub.status.idle":"2022-10-06T10:32:26.394671Z","shell.execute_reply.started":"2022-10-06T10:32:26.387914Z","shell.execute_reply":"2022-10-06T10:32:26.394072Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.395818Z","iopub.execute_input":"2022-10-06T10:32:26.396673Z","iopub.status.idle":"2022-10-06T10:32:26.452986Z","shell.execute_reply.started":"2022-10-06T10:32:26.396637Z","shell.execute_reply":"2022-10-06T10:32:26.452259Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model=NBMEModel()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:32:26.454213Z","iopub.execute_input":"2022-10-06T10:32:26.454731Z","iopub.status.idle":"2022-10-06T10:33:00.075150Z","shell.execute_reply.started":"2022-10-06T10:32:26.454686Z","shell.execute_reply":"2022-10-06T10:33:00.074454Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/712 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecdf83ea7d34e5aa619defd23a8f7fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929a53ce5bd3455c8e97e71ba3b00f25"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at Tsubasaz/clinical-bert-base-128 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at Tsubasaz/clinical-bert-base-128 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:33:00.076424Z","iopub.execute_input":"2022-10-06T10:33:00.076753Z","iopub.status.idle":"2022-10-06T10:33:05.076827Z","shell.execute_reply.started":"2022-10-06T10:33:00.076717Z","shell.execute_reply":"2022-10-06T10:33:05.076121Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"NBMEModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.4, inplace=False)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def train_fn(dataloader,model,optimizer,scheduler=None):\n    model.train()\n    \n    train_loss=0\n\n\n    tqd=tqdm(dataloader,total=len(dataloader))\n    \n    for batch , data in enumerate(tqd):\n\n      \n        ids=data['input_ids']\n        mask=data['mask']\n        token_ids=data['token_type_ids']\n        label=data['labels']\n        offsets=data['offsets']\n        \n        ids=ids.to(DEVICE,dtype=torch.long)\n        mask=mask.to(DEVICE,dtype=torch.long)\n        token_ids=token_ids.to(DEVICE,dtype=torch.long)\n        label=label.to(DEVICE,dtype=torch.long)\n        \n        model.zero_grad()\n        \n        output=model(ids=ids,mask=mask,token_ids=token_ids)\n        \n        loss=loss_fn(output,label)\n\n        train_loss=+loss.item()\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # cliping to get rid of exploding gradient if any \n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n    return train_loss/len(dataloader)\n        \n        \n           ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:33:05.078243Z","iopub.execute_input":"2022-10-06T10:33:05.078491Z","iopub.status.idle":"2022-10-06T10:33:05.105667Z","shell.execute_reply.started":"2022-10-06T10:33:05.078457Z","shell.execute_reply":"2022-10-06T10:33:05.104967Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_fn(dataloader, model):\n    model.eval()\n    \n        \n    eval_loss=0\n    \n    tk = tqdm(dataloader, total=len(dataloader)) \n    \n    for batch, data in enumerate(tk):\n        ids = data['input_ids']\n        token_type_ids = data[\"token_type_ids\"]\n        mask = data[\"mask\"]\n        labels = data['labels']\n        offsets = data[\"offsets\"]\n        ids = ids.to(DEVICE, dtype=torch.long)\n        token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n        mask = mask.to(DEVICE, dtype=torch.long)\n        labels = labels.to(DEVICE, dtype=torch.float64)\n      \n\n        logits = model(ids=ids, mask=mask, token_ids=token_type_ids ) #last_hidden_state\n            \n        loss = loss_fn(logits, labels)\n \n        eval_loss=+loss.item()\n         \n    \n        \n    return eval_loss/len(dataloader)\n      \n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:33:05.107276Z","iopub.execute_input":"2022-10-06T10:33:05.107830Z","iopub.status.idle":"2022-10-06T10:33:05.758969Z","shell.execute_reply.started":"2022-10-06T10:33:05.107793Z","shell.execute_reply":"2022-10-06T10:33:05.757988Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def run():\n    \n    train_loss_data, valid_loss_data = [], []\n    \n  \n    df_train , df_valid= train_test_split(df,test_size=0.3, random_state=42)\n   \n    df_train = df_train.reset_index(drop=True) \n    df_valid = df_valid.reset_index(drop=True)\n    \n    train_dataset = NBMEDataset(\n        pn_history=df_train.pn_history.values,\n        feature_text=df_train.feature_text.values,\n        annotation=df_train.annotation.values,\n        location=df_train.location.values\n        \n    )\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE\n \n    )\n    \n   \n\n    valid_dataset = NBMEDataset(\n        pn_history=df_valid.pn_history.values,\n        feature_text=df_valid.feature_text.values,\n        annotation=df_valid.annotation.values,\n        location=df_valid.location.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE\n    )\n\n\n    \n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n\n    for i in range(config.EPOCHS):\n        print(\"Epoch: {}/{}\".format(i + 1, config.EPOCHS))\n    \n      \n\n        train_loss = train_fn(train_data_loader, model, optimizer, scheduler=scheduler)\n        \n        eval_loss =  eval_fn(valid_data_loader , model )\n       \n        print(f\"Train loss: {train_loss} and the valida loss {eval_loss} after the epochs : {i+1}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:33:05.760622Z","iopub.execute_input":"2022-10-06T10:33:05.761137Z","iopub.status.idle":"2022-10-06T10:33:05.773894Z","shell.execute_reply.started":"2022-10-06T10:33:05.761100Z","shell.execute_reply":"2022-10-06T10:33:05.773101Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:33:05.775410Z","iopub.execute_input":"2022-10-06T10:33:05.775888Z","iopub.status.idle":"2022-10-06T10:45:07.410615Z","shell.execute_reply.started":"2022-10-06T10:33:05.775844Z","shell.execute_reply":"2022-10-06T10:45:07.409880Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch: 1/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fd1e8f05c5a4a1893061235c855e12b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/269 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d57bdd0703443797a9e5141c5e18bd"}},"metadata":{}},{"name":"stdout","text":"Train loss: 7.189687091512041e-05 and the valida loss 9.498140708886115e-05 after the epochs : 1\nEpoch: 2/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec6d4b765b7464eb35527a0dd5aac79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/269 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda7b682c528487b8e6d9a9a49467e8d"}},"metadata":{}},{"name":"stdout","text":"Train loss: 6.756610787524202e-05 and the valida loss 9.507552260138288e-05 after the epochs : 2\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(BASE_PATH + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.412262Z","iopub.execute_input":"2022-10-06T10:45:07.412681Z","iopub.status.idle":"2022-10-06T10:45:07.428803Z","shell.execute_reply.started":"2022-10-06T10:45:07.412643Z","shell.execute_reply":"2022-10-06T10:45:07.428169Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(test_df, features_df, on=['feature_num','case_num'], how='inner')\ndf_test =pd.merge(df_test, patient_notes_df, on=['pn_num','case_num'], how='inner')\n","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.430066Z","iopub.execute_input":"2022-10-06T10:45:07.430320Z","iopub.status.idle":"2022-10-06T10:45:07.448087Z","shell.execute_reply.started":"2022-10-06T10:45:07.430278Z","shell.execute_reply":"2022-10-06T10:45:07.447425Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.449135Z","iopub.execute_input":"2022-10-06T10:45:07.449520Z","iopub.status.idle":"2022-10-06T10:45:07.460589Z","shell.execute_reply.started":"2022-10-06T10:45:07.449484Z","shell.execute_reply":"2022-10-06T10:45:07.459833Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"          id  case_num  pn_num  feature_num  \\\n0  00016_000         0      16            0   \n1  00016_001         0      16            1   \n2  00016_002         0      16            2   \n3  00016_003         0      16            3   \n4  00016_004         0      16            4   \n\n                                        feature_text  \\\n0  Family-history-of-MI-OR-Family-history-of-myoc...   \n1                 Family-history-of-thyroid-disorder   \n2                                     Chest-pressure   \n3                              Intermittent-symptoms   \n4                                        Lightheaded   \n\n                                          pn_history  \n0  HPI: 17yo M presents with palpitations. Patien...  \n1  HPI: 17yo M presents with palpitations. Patien...  \n2  HPI: 17yo M presents with palpitations. Patien...  \n3  HPI: 17yo M presents with palpitations. Patien...  \n4  HPI: 17yo M presents with palpitations. Patien...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>feature_text</th>\n      <th>pn_history</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n      <td>Family-history-of-thyroid-disorder</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n      <td>Chest-pressure</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>Intermittent-symptoms</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n      <td>Lightheaded</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"> **Inference**","metadata":{}},{"cell_type":"code","source":"# Inferece on one of the data point request \n@torch.no_grad()\ndef inference_fn(df_test, model, device):\n    model.eval()\n    model.to(device)\n    \n\n    tokenized_input = config.TOKENIZER.encode_plus(df_test[0],df_test[1],return_attention_mask=True,padding='max_length',truncation=True,\n                                                return_offsets_mapping=True,return_token_type_ids=True,max_length=config.MAX_LEN)\n        \n    input_ids = torch.tensor(tokenized_input['input_ids'],dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(tokenized_input['attention_mask'],dtype=torch.long).unsqueeze(0)\n    token_type_ids = torch.tensor(tokenized_input['token_type_ids'],dtype=torch.long).unsqueeze(0)\n    offsets = tokenized_input['offset_mapping']\n        \n        \n    input_ids= input_ids.to(DEVICE)\n    mask=mask.to(DEVICE)\n    token_type_ids = token_type_ids.to(DEVICE)\n    \n    y_preds= model(input_ids,mask,token_type_ids)\n    \n    predictions= y_preds.sigmoid().to('cpu').numpy()\n    \n        \n    return predictions, offsets\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.461847Z","iopub.execute_input":"2022-10-06T10:45:07.462112Z","iopub.status.idle":"2022-10-06T10:45:07.470564Z","shell.execute_reply.started":"2022-10-06T10:45:07.462078Z","shell.execute_reply":"2022-10-06T10:45:07.469855Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"data,offsets=inference_fn(df_test.iloc[1,4:6],model,DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.471842Z","iopub.execute_input":"2022-10-06T10:45:07.472313Z","iopub.status.idle":"2022-10-06T10:45:07.503290Z","shell.execute_reply.started":"2022-10-06T10:45:07.472271Z","shell.execute_reply":"2022-10-06T10:45:07.502370Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"index_ofstring = np.where(data[0] >= 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.504518Z","iopub.execute_input":"2022-10-06T10:45:07.504771Z","iopub.status.idle":"2022-10-06T10:45:07.511209Z","shell.execute_reply.started":"2022-10-06T10:45:07.504739Z","shell.execute_reply":"2022-10-06T10:45:07.510265Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_test.iloc[1,4:6].values","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.512428Z","iopub.execute_input":"2022-10-06T10:45:07.512613Z","iopub.status.idle":"2022-10-06T10:45:07.521840Z","shell.execute_reply.started":"2022-10-06T10:45:07.512591Z","shell.execute_reply":"2022-10-06T10:45:07.520962Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array(['Family-history-of-thyroid-disorder',\n       'HPI: 17yo M presents with palpitations. Patient reports 3-4 months of intermittent episodes of \"heart beating/pounding out of my chest.\" 2 days ago during a soccer game had an episode, but this time had chest pressure and felt as if he were going to pass out (did not lose conciousness). Of note patient endorses abusing adderall, primarily to study (1-3 times per week). Before recent soccer game, took adderrall night before and morning of game. Denies shortness of breath, diaphoresis, fevers, chills, headache, fatigue, changes in sleep, changes in vision/hearing, abdominal paun, changes in bowel or urinary habits. \\r\\nPMHx: none\\r\\nRx: uses friends adderrall\\r\\nFHx: mom with \"thyroid disease,\" dad with recent heart attcak\\r\\nAll: none\\r\\nImmunizations: up to date\\r\\nSHx: Freshmen in college. Endorses 3-4 drinks 3 nights / week (on weekends), denies tabacco, endorses trying marijuana. Sexually active with girlfriend x 1 year, uses condoms'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# lets made it offsets \n\nbegning_of_string = offsets[187][0]\nend_of_string = offsets[188][1]","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.526046Z","iopub.execute_input":"2022-10-06T10:45:07.526288Z","iopub.status.idle":"2022-10-06T10:45:07.530511Z","shell.execute_reply.started":"2022-10-06T10:45:07.526248Z","shell.execute_reply":"2022-10-06T10:45:07.529593Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Model is doing good its able to predict properly \ndf_test.iloc[1,5][begning_of_string:end_of_string]","metadata":{"execution":{"iopub.status.busy":"2022-10-06T10:45:07.531749Z","iopub.execute_input":"2022-10-06T10:45:07.532203Z","iopub.status.idle":"2022-10-06T10:45:07.541180Z","shell.execute_reply.started":"2022-10-06T10:45:07.532168Z","shell.execute_reply":"2022-10-06T10:45:07.540420Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'thyroid disease'"},"metadata":{}}]}]}